### Minimum Description Length for Universal Science

Someone is broadcasting a stream of bits. You don't know why. A 500-bit-long sample looks like this:

```
01100110110101011011111100001001110000100011010001101011011010000001010000001010101001111010001011110101001001010100101010101010000101001101010100111111110101010101010101111111010101101010110111110101011011010101010000000110111110000011101011100000000000001111101010110101010101001010101101010101100111001100001100110101111111111111111111000110010110100110101010101011000000101010111011010100101100111111101011110111010001010101011100111101000110110101010110101100010110000010101010011001101010101111...
```

The thought occurs to you to [do Science to it](http://dresdencodak.com/2008/05/02/copan/)—to ponder if there's some way you could predict what bits are going to come next. At first you might think that you can't—it's just a bunch of random bits. You can't predict it; that's what random _means_.

Or does it? True, if the sequence represented flips of a fair coin—every flip independently landing either `0` or `1` with exactly equal probability—then there would be no way you could predict what would come next: any continuation you could posit would be exactly as probable as any other.

But if the sequence represented flips of a _biased_ coin—if, say, `1` came up 0.55 of the time instead of exactly 0.5—then it would be possible to predict better or worse. Your [best bet for the next bit in isolation would always be `1`](https://www.lesswrong.com/posts/msJA6B9ZjiiZxT6EZ/lawful-uncertainty), and you would more strongly anticipate sequences with slightly more `1`s than `0`s.

You count 265 `1`s in the sample of 500 bits. _Given_ the hypothesis that the bits were generated by a fair coin, the number of `1`s (or `0`s, without loss of generality) would be given by the binomial distribution ${500\choose k} (0.5)^k (0.5)^{500-k}$, which [has a standard deviation of](https://en.wikipedia.org/wiki/Binomial_distribution) $\sqrt{500 \cdot 0.5^2} = \sqrt{125} \approx 11.18$, so your observation of $265 - 250 = 15$ excess `1`s is about $15/11.18 \approx 1.34$ standard deviations from the mean—not out of the realm of plausibility of happening by chance, although you're certainly _suspicious_ that the coin behind these bits isn't quite fair.

... that is, if it's even a coin. You love talking in terms of shiny "coins" rather than stodgy old "[independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) binary-valued random variables", but looking at the sample again, you begin to _further_ doubt whether the bits are independent. You've [heard that humans are biased](https://www.lesswrong.com/posts/6xGC9P8wp2mi7uhti/inaccessible-finely-tuned-rng-in-humans) to overestimate the frequency of alternations (`10101`...) and underestimate the frequency of consecutive runs (`00000`... or `1111`...) in "truly" (uniformly) random data, but the 500-bit sample contains a

[TODO: use the long runs to motivate Markov chains, but then use the alternation runs to motivate higher-order Markov chains, but the question of what order to pick is what motivates penalizing complex models: lower-order models are the same as higher-order models with the coefficients pinned (third-order such that 010==110 &c. is just second-order)]

[TODO: you don't know it's "random"; it could be a radio signal of a movie, with runs where the sky is one color]

13 `0`s starting at position 243
19 `1`s starting at position 319
17 alternations starting at position 153


https://www.johndcook.com/blog/2012/11/14/probability-of-long-runs/
http://www.gregegan.net/QUARANTINE/Runs/Runs.html


[(Full source code.)](TODO)

### Bibliography

Peter D. Grünwald, _The Minimum Description Length Principle_

David J. C. MacKay, _Information Theory, Inference, and Learning Algorithms_, Ch. 28, "Model Comparison and Occam's Razor"
